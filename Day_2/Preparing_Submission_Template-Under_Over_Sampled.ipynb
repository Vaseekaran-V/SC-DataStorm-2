{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accurate-yahoo",
   "metadata": {},
   "source": [
    "### Importing the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eligible-feature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\niroshan\\miniconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\niroshan\\miniconda3\\lib\\site-packages (from imblearn) (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\niroshan\\miniconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\niroshan\\miniconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in c:\\users\\niroshan\\miniconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\niroshan\\miniconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\niroshan\\miniconda3\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dense-piece",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-bb824415200d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBalancedBaggingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#Import other necessary model libraries, for this example, using Logistic Regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler #you can use minmax scaler too\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "#Import other necessary model libraries, for this example, using Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-latex",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(r\"Data/Preprocessed-Datasets/train-data-encoded.csv\", header = 0)\n",
    "test_data = pd.read_csv(r\"../Data/Preprocessed-Datasets/test-data-encoded.csv\", header = 0)\n",
    "validation_data = pd.read_csv(r\"../Data/Preprocessed-Datasets/validation-data-encoded.csv\", header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "communist-forestry",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-faeb921daa00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Reservation-id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Reservation-id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Reservation-id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data=train_data.drop(['Reservation-id'], axis = 1)\n",
    "validation_data=validation_data.drop(['Reservation-id'], axis = 1)\n",
    "test_data=test_data.drop(['Reservation-id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting names of all the columns\n",
    "train_data_column_names = train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-berry",
   "metadata": {},
   "source": [
    "### Seperating the columns of categorical and quantitative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quan_cols = ['Age','Discount_Rate', 'Room_Rate','Expected_stay_days', \n",
    "                        'Reservation_gap', 'Adults', 'Children','Babies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat_cols = ['Gender', 'Ethnicity', 'Educational_Level', 'Income', 'Country_region',\n",
    "                 'Hotel_Type', 'Meal_Type', 'Visted_Previously',\n",
    "                 'Previous_Cancellations', 'Deposit_type', 'Booking_channel',\n",
    "                 'Required_Car_Parking', 'Use_Promotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols_plot = ['Age','Discount_Rate', 'Room_Rate','Expected_stay_days','Reservation_gap', \n",
    "             'Adults', 'Children','Babies', 'Gender', 'Ethnicity', 'Educational_Level', \n",
    "             'Income', 'Country_region','Hotel_Type', 'Meal_Type', 'Visted_Previously',\n",
    "             'Previous_Cancellations', 'Deposit_type', 'Booking_channel','Required_Car_Parking', \n",
    "             'Use_Promotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-islam",
   "metadata": {},
   "source": [
    "### Scaling the quantitative variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_quan = train_data[data_quan_cols]\n",
    "validation_data_quan = validation_data[data_quan_cols]\n",
    "test_data_quan = test_data[data_quan_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_quan_scaled = sc.fit_transform(train_data_quan)\n",
    "validation_data_quan_scaled = sc.transform(validation_data_quan)\n",
    "test_data_quan_scaled = sc.transform(test_data_quan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_quan_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_data_quan_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-respect",
   "metadata": {},
   "source": [
    "### Separating the categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-shuttle",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = data_quan_cols.copy()\n",
    "cols_to_drop.append(\"Reservation_Status\")\n",
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cat = train_data.drop(cols_to_drop, axis=1)\n",
    "validation_data_cat = validation_data.drop(cols_to_drop, axis=1)\n",
    "test_data_cat = test_data.drop(data_quan_cols, axis=1)\n",
    "train_data_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cat_array = train_data_cat.to_numpy()\n",
    "validation_data_cat_array = validation_data_cat.to_numpy()\n",
    "test_data_cat_array = test_data_cat.to_numpy()\n",
    "train_data_cat_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-ballot",
   "metadata": {},
   "source": [
    "### Joining the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed = np.concatenate((train_data_quan_scaled, train_data_cat_array), axis = 1)\n",
    "validation_data_processed = np.concatenate((validation_data_quan_scaled, validation_data_cat_array), axis = 1)\n",
    "test_data_processed = np.concatenate((test_data_quan_scaled, test_data_cat_array), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_quan_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_cat_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-villa",
   "metadata": {},
   "source": [
    "### Encoding the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_data['Reservation_Status']\n",
    "validation_y = validation_data['Reservation_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = le.fit_transform(train_y)\n",
    "validation_y = le.transform(validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-sussex",
   "metadata": {},
   "source": [
    "### Under and Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "#0: Cancelled, 1: Check--in , 2:No Show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling\n",
    "strategy = {0:4134*5, 1:21240, 2:2125*9}\n",
    "smote = SMOTE(sampling_strategy=strategy) \n",
    "\n",
    "X_smote, y_smote = smote.fit_resample(train_data_processed, train_y)\n",
    "#print(Counter(y_train), Counter(y_smote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NearMiss\n",
    "from imblearn.under_sampling import TomekLinks #use TomekLinks and NearMiss\n",
    "\n",
    "strategy = {1: round(21240*0.5), 0: 4134*2, 2: 2125*4}\n",
    "undersample = TomekLinks()\n",
    "\n",
    "X_near , y_near = undersample.fit_resample(X_smote,y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_near = np.bincount(y_near)\n",
    "print(unique_near)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-nutrition",
   "metadata": {},
   "source": [
    "Use X_near, y_near as points to plot data\n",
    "Use train_data_column_names for col_names\n",
    "Making a dataframe out of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed_df = pd.DataFrame(data = X_near , columns = train_data_column_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
